# patch-modules.ps1
param([string]$ProjectRoot = (Get-Location).Path)

function Write-Text {
  param([string]$Path, [string[]]$Lines)
  [System.IO.Directory]::CreateDirectory((Split-Path $Path -Parent)) | Out-Null
  [System.IO.File]::WriteAllLines($Path, $Lines)
  Write-Host "Wrote: $Path"
}

Push-Location $ProjectRoot

# --- ok_mvp\youtube_module.py ---
$yt = @(
  'import json',
  'from pathlib import Path',
  'from typing import List, Dict',
  '',
  'from yt_dlp import YoutubeDL',
  'from youtube_transcript_api import (',
  '    YouTubeTranscriptApi,',
  '    TranscriptsDisabled,',
  '    NoTranscriptFound,',
  '    VideoUnavailable,',
  ')',
  'from .logger import get_logger',
  'from .config import TOP_N_YOUTUBE_VIDEOS',
  'from .llm_utils import chunk_text, call_llm',
  '',
  'logger = get_logger()',
  '',
  'def _search_videos(query: str, limit: int) -> List[Dict]:',
  '    ydl_opts = {',
  '        "quiet": True,',
  '        "skip_download": True,',
  '        "extract_flat": True,',
  '        "noplaylist": True',
  '    }',
  '    with YoutubeDL(ydl_opts) as ydl:',
  '        info = ydl.extract_info(f"ytsearch{limit}:{query}", download=False)',
  '    entries = info.get("entries", []) if isinstance(info, dict) else []',
  '    out = []',
  '    for e in entries:',
  '        if not isinstance(e, dict):',
  '            continue',
  '        vid = e.get("id") or ""',
  '        title = e.get("title") or ""',
  '        url = e.get("webpage_url") or e.get("url") or (f"https://www.youtube.com/watch?v={vid}" if vid else "")',
  '        out.append({"video_id": vid, "title": title, "link": url})',
  '    return out',
  '',
  'def _fetch_transcript_text(video_id: str) -> str:',
  '    try:',
  '        # Prefer English, but let API fall back to auto-generated if needed',
  '        lines = YouTubeTranscriptApi.get_transcript(video_id, languages=["en", "en-US", "en-GB"])',
  '        return " ".join(chunk.get("text", "") for chunk in lines)',
  '    except (TranscriptsDisabled, NoTranscriptFound, VideoUnavailable) as e:',
  '        logger.warning(f"Transcript unavailable for {video_id}: {e}")',
  '        return ""',
  '    except Exception as e:',
  '        logger.warning(f"Transcript error for {video_id}: {e}")',
  '        return ""',
  '',
  'def fetch_and_analyze_youtube(topic: str):',
  '    logger.info(f"[YouTube] Topic: {topic} | TopN={TOP_N_YOUTUBE_VIDEOS}")',
  '    results = _search_videos(topic, TOP_N_YOUTUBE_VIDEOS)',
  '    videos = []',
  '    corpus_parts = []',
  '    for r in results:',
  '        title = r.get("title", "")',
  '        link = r.get("link", "")',
  '        vid = r.get("video_id", "")',
  '        transcript = _fetch_transcript_text(vid) if vid else ""',
  '        if transcript:',
  '            corpus_parts.append(transcript)',
  '        videos.append({',
  '            "title": title,',
  '            "link": link,',
  '            "video_id": vid,',
  '            "has_transcript": bool(transcript),',
  '            "transcript_chars": len(transcript) if transcript else 0',
  '        })',
  '',
  '    corpus = "\\n\\n".join(corpus_parts)',
  '    chunks = chunk_text(corpus)',
  '    prompt = "Summarize the core insights, trends, and actionable opportunities from these YouTube transcripts. Output concise bullet points suitable for an Opportunity Brief."',
  '    summary = call_llm(prompt, chunks)',
  '',
  '    out = {',
  '        "topic": topic,',
  '        "source": "youtube",',
  '        "top_n": TOP_N_YOUTUBE_VIDEOS,',
  '        "videos": videos,',
  '        "summary": summary',
  '    }',
  '    out_path = Path("output") / f"{topic.replace('' '', ''_'')}_youtube.json"',
  '    out_path.parent.mkdir(parents=True, exist_ok=True)',
  '    with open(out_path, "w", encoding="utf-8") as f:',
  '        json.dump(out, f, indent=2)',
  '    logger.info(f"[YouTube] Wrote {out_path}")'
)
Write-Text "$ProjectRoot\ok_mvp\youtube_module.py" $yt

# --- ok_mvp\arxiv_module.py ---
$ax = @(
  'import json',
  'from pathlib import Path',
  'import io',
  'from typing import List, Dict',
  '',
  'import arxiv',
  'import requests',
  'from pypdf import PdfReader',
  'from .logger import get_logger',
  'from .config import TOP_N_ARXIV_PAPERS',
  'from .llm_utils import chunk_text, call_llm',
  '',
  'logger = get_logger()',
  '',
  'def _search_arxiv(query: str, limit: int) -> List[Dict]:',
  '    search = arxiv.Search(query=query, max_results=limit, sort_by=arxiv.SortCriterion.Relevance)',
  '    return list(search.results())',
  '',
  'def _download_pdf_bytes(entry) -> bytes:',
  '    pdf_url = getattr(entry, "pdf_url", "")',
  '    if not pdf_url:',
  '        return b""',
  '    try:',
  '        r = requests.get(pdf_url, timeout=30)',
  '        if r.status_code == 200 and r.content:',
  '            return r.content',
  '        logger.warning(f"PDF HTTP {r.status_code} for {getattr(entry, ''title'', ''unknown'')}")',
  '        return b""',
  '    except Exception as e:',
  '        logger.warning(f"PDF download failed for {getattr(entry, ''title'', ''unknown'')}: {e}")',
  '        return b""',
  '',
  'def _pdf_to_text(pdf_bytes: bytes) -> str:',
  '    if not pdf_bytes:',
  '        return ""',
  '    try:',
  '        reader = PdfReader(io.BytesIO(pdf_bytes))',
  '        texts = []',
  '        for page in reader.pages:',
  '            try:',
  '                texts.append(page.extract_text() or "")',
  '            except Exception:',
  '                texts.append("")',
  '        return "\\n".join(texts)',
  '    except Exception as e:',
  '        logger.warning(f"PDF parse failed: {e}")',
  '        return ""',
  '',
  'def fetch_and_analyze_arxiv(topic: str):',
  '    logger.info(f"[arXiv] Topic: {topic} | TopN={TOP_N_ARXIV_PAPERS}")',
  '    entries = _search_arxiv(topic, TOP_N_ARXIV_PAPERS)',
  '    papers = []',
  '    per_paper_summaries = []',
  '    for e in entries:',
  '        title = getattr(e, "title", "")',
  '        authors = [a.name for a in getattr(e, "authors", [])]',
  '        pdf_bytes = _download_pdf_bytes(e)',
  '        text = _pdf_to_text(pdf_bytes)',
  '        chunks = chunk_text(text)',
  '        prompt = "Summarize this paper for a product strategist. Extract problem, methods, key findings, and potential business applications."',
  '        summary = call_llm(prompt, chunks) if chunks else "No text extracted."',
  '        per_paper_summaries.append({"title": title, "summary": summary})',
  '        papers.append({',
  '            "title": title,',
  '            "authors": authors,',
  '            "entry_id": getattr(e, "entry_id", ""),',
  '            "pdf_url": getattr(e, "pdf_url", ""),',
  '            "extracted_chars": len(text)',
  '        })',
  '',
  '    combined = "\\n\\n".join(s["summary"] for s in per_paper_summaries if s.get("summary"))',
  '    final = call_llm(',
  '        "Synthesize cross-paper insights into concrete market opportunities and risks. Be concise, actionable, and specific.",',
  '        chunk_text(combined)',
  '    ) if combined else "No summaries produced."',
  '',
  '    out = {',
  '        "topic": topic,',
  '        "source": "arxiv",',
  '        "top_n": TOP_N_ARXIV_PAPERS,',
  '        "papers": papers,',
  '        "per_paper_summaries": per_paper_summaries,',
  '        "synthesis": final',
  '    }',
  '    out_path = Path("output") / f"{topic.replace('' '', ''_'')}_arxiv.json"',
  '    out_path.parent.mkdir(parents=True, exist_ok=True)',
  '    with open(out_path, "w", encoding="utf-8") as f:',
  '        json.dump(out, f, indent=2)',
  '    logger.info(f"[arXiv] Wrote {out_path}")'
)
Write-Text "$ProjectRoot\ok_mvp\arxiv_module.py" $ax

Pop-Location
Write-Host "Patched youtube_module.py and arxiv_module.py"
